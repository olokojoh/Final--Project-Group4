# -*- coding: utf-8 -*-
"""Rewrite_GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rN17Wa3_UtBQRC1zMbtiZCh2TU1yyzPj
"""

import os
import cv2
import sys
import math
import time
import glob
import random
import warnings
import datetime
import itertools
import numpy as np
import pandas as pd
from PIL import Image
from tqdm.auto import tqdm
from google.colab import drive
from matplotlib import pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from torchvision import datasets
from torch.autograd import Variable
from torchvision.utils import save_image
from torch.utils.data import DataLoader,Dataset

epoch = 0
n_epochs=100
dataset_name='test'
batch_size = 16
lr = 0.0005
b1 = 0.5
b2 = 0.999
decay_epoch = 100
n_cpu = 4
img_height = 256
img_width = 256
channels = 3
sample_interval = 100
checkpoint_interval = 338

warnings.filterwarnings('ignore')
DATA_DIR = os.getcwd()
data_path =  '../Data/Train_2'#try to train Train_2 at first in this case, but actually, the result showed in the report was the result from all data set.
#train_list = [f for f in os.listdir(data_path) if f[:-2] == "Train"]

#In this class, root is the path of the file.
class ImageDataset_color(Dataset):
  def __init__(self,root,transforms_=None,mode="train"):
    self.transform = transforms.Compose(transforms_)#transform
    self.files = sorted(glob.glob(root+"/*.*"))#dir
  def __getitem__(self,index):
    img_A = Image.fromarray(np.array(cv2.cvtColor(cv2.imread(self.files[index%len(self.files)]),cv2.COLOR_BGR2RGB)), "RGB")
    img_B = Image.fromarray(np.array(cv2.cvtColor(cv2.cvtColor(cv2.cvtColor(cv2.imread(self.files[index%len(self.files)]),cv2.COLOR_BGR2RGB),cv2.COLOR_RGB2GRAY),cv2.COLOR_GRAY2RGB)), "RGB")
    img_A = self.transform(img_A)
    img_B = self.transform(img_B)       
    return {"A":img_A,"B":img_B}
  def __len__(self):
    return len(self.files)

transforms_=[transforms.Resize((256,256), Image.BICUBIC),transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]
dataloader=DataLoader(ImageDataset_color(data_path,transforms_=transforms_),batch_size=16,shuffle=True,num_workers=4,)
val_dataloader=DataLoader(ImageDataset_color(data_path,transforms_=transforms_,mode="test"),batch_size=10,shuffle=True,num_workers=1,)

def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find("BatchNorm2d") != -1:
        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
        torch.nn.init.constant_(m.bias.data, 0.0)
#U-NET
class UNetDown(nn.Module):
  def __init__(self, in_size, out_size, normalize=True, dropout=0.0):
      super(UNetDown, self).__init__()
      layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]
      if normalize:
          layers.append(nn.InstanceNorm2d(out_size))
      layers.append(nn.LeakyReLU(0.2))
      if dropout:
          layers.append(nn.Dropout(dropout))
      self.model = nn.Sequential(*layers)
  def forward(self, x):
      return self.model(x)
class UNetUp(nn.Module):
  def __init__(self, in_size, out_size, dropout=0.0):
      super(UNetUp, self).__init__()
      layers = [nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),nn.InstanceNorm2d(out_size),nn.ReLU(inplace=True),]
      if dropout:
          layers.append(nn.Dropout(dropout))
      self.model = nn.Sequential(*layers)
  def forward(self, x, skip_input):
      x = self.model(x)
      x = torch.cat((x, skip_input), 1)
      return x
class GeneratorUNet(nn.Module):
  def __init__(self, in_channels=3, out_channels=3):
      super(GeneratorUNet, self).__init__()
      self.down1 = UNetDown(in_channels, 64, normalize=False)
      self.down2 = UNetDown(64, 128)
      self.down3 = UNetDown(128, 256)
      self.down4 = UNetDown(256, 512, dropout=0.5)
      self.down5 = UNetDown(512, 512, dropout=0.5)
      self.down6 = UNetDown(512, 512, dropout=0.5)
      self.down7 = UNetDown(512, 512, dropout=0.5)
      self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)
      self.up1 = UNetUp(512, 512, dropout=0.5)
      self.up2 = UNetUp(1024, 512, dropout=0.5)
      self.up3 = UNetUp(1024, 512, dropout=0.5)
      self.up4 = UNetUp(1024, 512, dropout=0.5)
      self.up5 = UNetUp(1024, 256)
      self.up6 = UNetUp(512, 128)
      self.up7 = UNetUp(256, 64)
      self.final = nn.Sequential(
          nn.Upsample(scale_factor=2),
          nn.ZeroPad2d((1, 0, 1, 0)),
          nn.Conv2d(128, out_channels, 4, padding=1),
          nn.Tanh(),)
  def forward(self, x):
      # U-Net generator with skip connections from encoder to decoder
      d1 = self.down1(x)
      d2 = self.down2(d1)
      d3 = self.down3(d2)
      d4 = self.down4(d3)
      d5 = self.down5(d4)
      d6 = self.down6(d5)
      d7 = self.down7(d6)
      d8 = self.down8(d7)
      u1 = self.up1(d8,d7)
      u2 = self.up2(u1,d6)
      u3 = self.up3(u2,d5)
      u4 = self.up4(u3,d4)
      u5 = self.up5(u4,d3)
      u6 = self.up6(u5,d2)
      u7 = self.up7(u6,d1)
      return self.final(u7)
#Discriminator
class Discriminator(nn.Module):
    def __init__(self, in_channels=3):
        super(Discriminator, self).__init__()
        def discriminator_block(in_filters, out_filters, normalization=True):
            """Returns downsampling layers of each discriminator block"""
            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]
            if normalization:
                layers.append(nn.InstanceNorm2d(out_filters))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers
        self.model = nn.Sequential(
            *discriminator_block(in_channels * 2, 64, normalization=False),
            *discriminator_block(64, 128),
            *discriminator_block(128, 256),
            *discriminator_block(256, 512),
            nn.ZeroPad2d((1, 0, 1, 0)),
            nn.Conv2d(512, 1, 4, padding=1, bias=False))
    def forward(self, img_A, img_B):
        # Concatenate image and condition image by channels to produce input
        img_input = torch.cat((img_A, img_B), 1)
        return self.model(img_input)

cuda = True if torch.cuda.is_available() else False
Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor
# Loss functions
criterion_GAN = torch.nn.MSELoss()
criterion_pixelwise = torch.nn.L1Loss()
# Loss weight of L1 pixel-wise loss between translated image and real image
lambda_pixel = 100 #

# Calculate output of image discriminator (PatchGAN)
patch = (1,16,16) #(1,16,16),img_height // 2 ** 4, img_width // 2 ** 4
# Initialize generator and discriminator
generator = GeneratorUNet()
discriminator = Discriminator()
#if GPU is avaiable
if cuda==True:
  generator = generator.cuda()
  discriminator = discriminator.cuda()
  criterion_GAN.cuda()
  criterion_pixelwise.cuda()
if epoch == 0:
  generator.apply(weights_init_normal)
  discriminator.apply(weights_init_normal)
else:
  generator.load_state_dict(torch.load(DATA_DIR+"/generator_%d.pth"%epoch))
  print("generator_%d.pth"%epoch + "loaded")
  discriminator.load_state_dict(torch.load(DATA_DIR+"/discriminator_%d.pth"%epoch))

optimizer_G = torch.optim.Adam(generator.parameters(),lr=0.0005,betas=(b1,b2))
optimizer_D = torch.optim.Adam(discriminator.parameters(),lr=0.0005,betas=(b1,b2))

def sample_images(batches_done):
  imgs = next(iter(val_dataloader))
  real_A = Variable(imgs["B"].type(Tensor))
  real_B = Variable(imgs["A"].type(Tensor))
  fake_B = generator(real_A)
  img_sample = torch.cat((real_A.data,fake_B.data, real_B.data),-2).cpu().numpy().astype(np.float32)
  img_sample -=img_sample.min()
  img_sample/=img_sample.max()
  img_sample = img_sample.transpose(0,2,3,1)
  plt.figure(figsize=[10,20])
  for row in range(5):
      plt.subplot(1,5,row+1)
      plt.imshow(img_sample[row])
  plt.show()

prev_time = time.time()
loss_G_plot=[]
loss_D_plot=[]
for epoch in range(0,n_epochs):
  print(epoch)
  for i, batch in enumerate(dataloader):
    # Model inputs
    real_A = Variable(batch["B"].type(Tensor))
    real_B = Variable(batch["A"].type(Tensor))
    # Adversarial ground truths
    valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)
    fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)
    #Train Generators
    optimizer_G.zero_grad()
    # GAN loss
    fake_B = generator(real_A)
    pred_fake = discriminator(fake_B, real_A)
    loss_GAN = criterion_GAN(pred_fake, valid)
    # Pixel-wise loss
    loss_pixel = criterion_pixelwise(fake_B,real_B)
    # Total loss
    loss_G =lambda_pixel * loss_pixel+loss_GAN
    loss_G_plot.append(loss_G)
    loss_G.backward()
    optimizer_G.step()
    # Train Discriminator
    optimizer_D.zero_grad()
    # Real loss
    pred_real = discriminator(real_B,real_A)
    loss_real = criterion_GAN(pred_real,valid)
    # Fake loss
    pred_fake = discriminator(fake_B.detach(),real_A)
    loss_fake = criterion_GAN(pred_fake, fake)
    # Total loss
    loss_D = 0.5 * (loss_real + loss_fake)
    loss_D_plot.append(loss_D)
    loss_D.backward()
    optimizer_D.step()
    # --------------
    #  Log Progress
    # --------------
    # Determine approximate time left
    batches_done=epoch*len(dataloader) + i
    batches_left=n_epochs*len(dataloader)-batches_done
    time_left = datetime.timedelta(seconds=batches_left*(time.time()-prev_time))
    prev_time = time.time()
    # Print log
    sys.stdout.write("\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s"%(epoch,n_epochs,i,len(dataloader),loss_D.item(),loss_G.item(),loss_pixel.item(),loss_GAN.item(),time_left,))
    # If at sample interval save image
    if batches_done % sample_interval==0:
      sample_images(batches_done)
  #if checkpoint_interval != -1 and epoch % checkpoint_interval==0:
  torch.save(generator.state_dict(),DATA_DIR+"/generator_%d.pth"%epoch)
  print("generator_%d.pth"%epoch + "saved")
  torch.save(discriminator.state_dict(),DATA_DIR+"/discriminator_%d.pth"%epoch)
