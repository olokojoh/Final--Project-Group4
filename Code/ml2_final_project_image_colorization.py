# -*- coding: utf-8 -*-
"""ML2 final project - Image colorization

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g2b-SPHVowR2_IQuZ1GFrcST3b9ou36v

<b>

<p>
<center>
<font size="5">
Machine Learning II - Final Project
</font>
</center>
</p>

<p>
<center>
<font size="4">
Image colorization - General Adversarial Network
</font>
</center>
</p>

<p>
<center>
<font size="3">
Data Science, Columbian College of Arts & Sciences, George Washington University
</font>
</center>
</p>

<p>
<center>
<font size="3">
Author: Jialei Chen, Chengrui Xu, Ruijin Jia
</font>
</center>
</p>

</b>

## Google Drive
"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Get absolute path
abspath = '/content/drive/My Drive/Colab Notebooks/ml2_final_project/'

"""## Warning"""

import warnings

# Ignore warnings
warnings.filterwarnings('ignore')

import cv2

"""## Load dataset"""

import os

DATA_DIR = os.getcwd()

3%2

data_path =  abspath + 'Flickr1024 Dataset'
train_list = [f for f in os.listdir(data_path) if f[:-2] == "Train"]
x = []

for index1, path in enumerate(train_list):
  for index2, image in enumerate([f for f in os.listdir(data_path+'/'+path)]):
    # print(data_path + '/'+ image)
    x.append(cv2.imread(data_path + '/'+ path+ '/'+ image))
    print("loading %d dataset:" % (index1))
    if index%100 == 0:
      print("%d images loaded" %(index2))

train_list

import numpy as np

dim=np.vstack(([i.shape[0] for i in x],[i.shape[1] for i in x],[i.shape[2] for i in x])).T
# for i in range(10):
#   print(x[i].shape)
#   print(x[i].shape[0]*x[i].shape[1]*x[i].shape[2])
#   print('---------------------------')
print(dim[:,0].min(),'-',dim[:,0].max())
print(dim[:,1].min(),'-',dim[:,1].max())
print(dim[:,2].min(),'-',dim[:,2].max())

train_list

from google.colab.patches import cv2_imshow
img1 = x[250]
img1_yuv = cv2.cvtColor(img1, cv2.COLOR_BGR2YUV)

cv2_imshow(img1_yuv[:,:,0])
cv2_imshow(img1_yuv[:,:,1])
cv2_imshow(img1_yuv[:,:,2])

img1_yuv.shape

y, u, v = cv2.split(img1_yuv)

# Commented out IPython magic to ensure Python compatibility.
train_ds = PairImageDataset(args.training_dir)
logging.info('loaded dataset from: {}, data length: {}'.format(args.training_dir, train_ds.__len__()))
train_dataloader = data.DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=0)
​
i = 0
adversarial_loss = torch.nn.BCELoss()
optimizer_G = torch.optim.Adam(G.parameters(), lr=args.g_lr, betas=(0.5, 0.999))
optimizer_D = torch.optim.Adam(D.parameters(), lr=args.d_lr, betas=(0.5, 0.999))

    for epoch in range(start_epoch, args.epoch):
        for i, (y, uv) in enumerate(train_dataloader):
            try:
                # Adversarial ground truths
                valid = Variable(torch.Tensor(y.size(0), 1).fill_(1.0),
                                requires_grad=False).to(device)
                fake = Variable(torch.Tensor(y.size(0), 1).fill_(0.0),
                                requires_grad=False).to(device)
​
                yvar = Variable(y).to(device)
                uvvar = Variable(uv).to(device)
                real_imgs = torch.cat([yvar, uvvar], dim=1)
​
                optimizer_G.zero_grad()
                uvgen = G(yvar)
                # Generate a batch of images
                gen_imgs = torch.cat([yvar.detach(), uvgen], dim=1)
​
                # Loss measures generator's ability to fool the discriminator
                g_loss_gan = adversarial_loss(D(gen_imgs), valid)
                g_loss = g_loss_gan + args.pixel_loss_weights * torch.mean(
                    (uvvar - uvgen)**2)
                if i % args.g_every == 0:
                    g_loss.backward()
                    optimizer_G.step()
​
                optimizer_D.zero_grad()
                # Measure discriminator's ability to classify real from generated samples
                real_loss = adversarial_loss(D(real_imgs), valid)
                fake_loss = adversarial_loss(D(gen_imgs.detach()), fake)
                d_loss = (real_loss + fake_loss) / 2
                d_loss.backward()
                optimizer_D.step()
                if i % 300 == 0:
                    logging.info("Epoch: %d, iter: %d, D loss: %f, G total loss: %f, GAN Loss: %f"
#                         % (epoch, i, d_loss.item(), g_loss.item(), g_loss_gan.item()))
                    save_weights(
                        {'D': D.state_dict(), 'G': G.state_dict(), 'epoch': epoch},
                        epoch
                    )
                    
                    # snap some images from dir
                    test_imgs = glob.glob('images/*.jpeg')
                    for test_img in test_imgs:
                        snap_image_result_from_file(test_img, G)
​
            except KeyboardInterrupt:
                logging.info('interrupted. try saving model now..')
                save_weights(
                    {'D': D.state_dict(), 'G': G.state_dict(), 'epoch': epoch}, 0
                )
                logging.info('saved.')
                exit(0)